{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "30071801-4b6b-4d28-9826-a627087b160a"
    }
   },
   "source": [
    "# Vorverarbeitung von Texten mit Python und NLTK\n",
    "\n",
    "\n",
    "Für viele Aufgaben müssen Texte immer auf der gleichen Art analysiert werden. Für eine Aufgabe wie die Sprachidentifikation können wir einen Text als eine lange Zeichenkette betrachten. Meistens brauchen wir aber eine Liste von Wörtern, mit denen wir weiter arbeiten können. \n",
    "\n",
    "Wenn wir erstmal den Text haben (was oft nicht einfach ist, wenn der Text z.B. aus einer Webseite extrahiert werden muss), teilen wir den Text zuerst in Sätze und die Sätze anschließend in Wörtern auf. In vielen Fällen führen wir die Wörter dann noch auf ihren Grundform zurück, und bestimmen die Wortart für jedes Wort, da diese oft wichtige Informationen für die wietere Verarbeitung gibt. Eventuell folgen dann Schritte, wie das Nachschlagen der Wörter in einem Thesaurus oder das Erkennen von sogenannten _Named Entities_: Namen von Personen, Institutionen, Produkte, usw.\n",
    "\n",
    "![Typische Verarbeitungsschritte bei der Textanalyse](http://textmining.wp.hs-hannover.de/img/pipeline.jpg)\n",
    "\n",
    "Für die Analyse von Englischen Texten sind weitaus mehr Werkzeuge verfügbar als für das Deutsche. Die Verarbeitung von englischen Texten ist daher etwas einfacher. Das wir außerdem auch of englische Texte analysieren müssen, schauen wir uns die Standardverarbeitung zunächst für das Englische an. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "28f5a8ff-008f-4595-a04b-f7fd4fc27c83"
    }
   },
   "source": [
    "## Englische Texte\n",
    "\n",
    "Wenn wir mit Python Texte analysieren wollen, ist das Paket NLTK der Stanford University unverzichtbar. Dieses Paket umfasst State-of-the-Art Implementierungen für so gut wie alle wichtige Algorithmen aus der Sprachverarbeitung und ist in den meisten Python-Distributionen enthalten.\n",
    "\n",
    "NLTK enthält auch eine große Menge Ressourcen, die Sie nutzen können, oder die manche der bereitgestellten Algorithmen brauchen. Diese sind meistens noch nicht installiert. Das nachinstallieren dieser Ressourcen ist ganz einfach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk #natural language toolkit\n",
    "\n",
    " nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sollte jetz ein Fernster geöffnet werden. Wählen Sie alles aus dem NLTK-Buch zum installieren aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0a1ea8c-f8b7-4969-9623-c7e338bdc707"
    }
   },
   "source": [
    "### Satzerkennung und Tokenization \n",
    "\n",
    "Mit der Python-Funktion _Split()_ können wir einen Text leicht aufteilen. In vielen Fällen machen wir dann aber Fehler. Es fängt schon damit an, dass Satzzeichen am vorangehenden Wort geschrieben werden, aber nicht dazu gehören, es sei denn, das Wort ist eine Abkürzung oder eine Ordinalzahl, aber letzteres nur im Deutschen. Statt uns über alle Ausnahmen Gedanken zu machen, nutzen wir hierfür einfach eine Funktion aus NLTK. Diese Funktion ist übrigens nicht auf Regeln basiert, sondern wurde aus vielen Beispielen gelernt. \n",
    "\n",
    "Im nächsten Beispiel sehen wir, wie man mit Python und NLTK eine Zeichenkette in eine Liste von Wörtern aufteilen kann. Wir finden nicht nur Wörter, sondern auch Satzzeichen, Zahlen und Symbole. Der Sammelbegriff für diese Einheiten ist _Token_. Das Zerlegen einer Zeichenkette in Tokens wird daher Tokenisierung oder auf Englisch _Tokenization_ genannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:31.475200Z",
     "start_time": "2017-10-11T23:41:31.467200Z"
    },
    "nbpresent": {
     "id": "d81160be-4c5c-4508-8438-fbaaa11474be"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence= \"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\n",
    "tokens = nltk.word_tokenize(sentence, language='english') #'german'\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vielen Fällen ist es wichtig zu wissen, wo die Satzgrenzen sind: Die Wörter in einem Satz haben eine viel engere Beziehung zueinander, als Wörter in verscheidenen Sätzen. Zum Beispiel stehen Begriffe, die aus mehreren Wörtern bestehen, wie \"Bundesministerium für Gesundheit\" immer innerhalb eines Satzes.\n",
    "\n",
    "Die Frage ist nun, ob wir den Text erst in Wörter aufteilen und dann in Sätzen oder umgekehrt. Um zu entscheiden, ob ein Punkt ein Satzende markiert, muss man unter anderem Wissen, ob das Wort vor dem Punkt eine Abkürzung ist. Man muss das Wort also schon mal in der langen Zeichenkette erkannt haben. Der _Sentence Splitter_ von NLTK arbeitet aber trotzdem auf ganzen Texten und braucht keine vorangehende Zerlegung in Tokens. \n",
    "\n",
    "Zum Ausprobieren, lesen wir einen kurzen englischen Text ein. Den hier genutzten Beispieltext finden Sie hier: http://textmining.wp.hs-hannover.de/texte/hanover.txt . Der Text ist der Wikipediaseite http://en.wikipedia.org/wiki/Hanover.html entnommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "l=[1,2,3,4,5,6]\n",
    "# for e in l:\n",
    "#     if e%2==0:\n",
    "#         print(e)\n",
    "gerade=[e for e in l if e==3]\n",
    "print(gerade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:33.353200Z",
     "start_time": "2017-10-11T23:41:33.325200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hanover', 'or', 'Hannover', '(', '/ˈhænoʊvər/', ';', 'German', ':', 'Hannover', ',', 'pronounced', '[', 'haˈnoːfɐ', ']', '(', 'About', 'this', 'sound', 'listen', ')', ')', ',', 'on', 'the', 'River', 'Leine', ',', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'the', 'German', 'state', 'of', 'Lower', 'Saxony', '(', 'Niedersachsen', ')', ',', 'and', 'was', 'once', 'by', 'personal', 'union', 'the', 'family', 'seat', 'of', 'the', 'Hanoverian', 'Kings', 'of', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Ireland', ',', 'under', 'their', 'title', 'as', 'the', 'dukes', 'of', 'Brunswick-Lüneburg', '(', 'later', 'described', 'as', 'the', 'Elector', 'of', 'Hanover', ')', '.'], ['At', 'the', 'end', 'of', 'the', 'Napoleonic', 'Wars', ',', 'the', 'Electorate', 'was', 'enlarged', 'to', 'become', 'a', 'Kingdom', 'with', 'Hanover', 'as', 'its', 'capital', '.'], ['From', '1868', 'to', '1946', 'Hanover', 'was', 'the', 'capital', 'of', 'the', 'Prussian', 'Province', 'of', 'Hanover', 'and', 'afterwards', 'of', 'the', 'Hanover', 'administrative', 'region', 'until', 'that', 'was', 'abolished', 'in', '2005', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "textfile = codecs.open(\"texte/hanover.txt\", \"r\", \"utf-8-sig\")\n",
    "text = textfile.read()\n",
    "textfile.close()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text,language='english') #Liste Sätze\n",
    "\n",
    "#print(sentences)\n",
    "\n",
    "tokenized_text = [nltk.word_tokenize(sent, language='english') for sent in sentences]\n",
    "\n",
    "print(tokenized_text[0:3])\n",
    "#print(tokenized_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wortarterkennung (Part-of-Speech Tagging)\n",
    "\n",
    "Die Wortart eines Wortes gibt oft wichtige Informationen. Den Hauptinhalt eines Textes erkennen wir beispielsweise schon an den benutzten Substantiven und Verben, während Adjektive und Adverbien wenig beitragen, und Artikel, Präpositionen und Hilfsverben hierzu überhaupt keine nützliche Information liefern. \n",
    "\n",
    "Wortarten werden im Englischen _Part of Speech_ genannt, ein Programm, dass die Wortarten zuweist, daher _Part of speaach tagger_ oder einfach _POS tagger_. Der NLTK enthält einen guten (statistischen) POS Tagger.\n",
    "\n",
    "Die Wortklassen, die dieser Tagger zuweist, sind nicht genau die, die Sie in der Schule gelernt haben. Manche Klassen sind unterteilt, es gibt Tags, die neben der Wortklasse weitere Informationen, wie z.B. 3. Person Singular enthalten und es gibt Klassen für Wörter, die oft schwierig einzuteilen sind. Die Tags, die im folgenden Beispiel genutzt werden, sind die aus dem sogenannten Pennsylvenia Treebank Tagset. Eine Beschreibung der Tags sowie Beispiel dafür bekommen Sie mit der help Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:35.272200Z",
     "start_time": "2017-10-11T23:41:35.246200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T12:48:20.712587Z",
     "start_time": "2017-10-11T12:48:20.703587Z"
    }
   },
   "source": [
    "Jetzt aber ein Beispiel für den Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T23:41:37.305200Z",
     "start_time": "2017-10-11T23:41:37.182200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haˈnoːfɐ\n",
      "sound\n",
      "capital\n",
      "city\n",
      "state\n",
      "union\n",
      "family\n",
      "seat\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_text[0])\n",
    "\n",
    "for (word,tag) in tags:\n",
    "    if tag=='NN':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung\n",
    "\n",
    "In vielen Sprachen, wie auch im Deutschen und Englischen, können Wörter in verschiedenen Formen auftreten. (Es gibt auch Sprachen, in denen das nicht der Fall ist. Diese Sprachen werden isolierende Sprachen genannt. Beispiele hierfür sind Mandarin (Chinesisch) und Vietnamesisch)). Oft ist es wichtig, den Grundfom eines Wortes, das im Text in flektierter Form vorkommt, zu bestimmen. Es ist wichtig, dass wir hier drei Begriffe klar trennen:\n",
    "* Lemma - Die Form des Wortes, wie sie in einem Wörterbuch steht. Z.B.: Haus, laufen, begründen\n",
    "* Stamm - Das Wort ohne Flexionsendungen (Prefixe und Suffixe). Z.B.: Haus, lauf, begründ\n",
    "* Wurzel - Kern des Wortes, von dem das Wort ggf. durch Derivation abgeleitet wurde. Z.B.: Haus, lauf, Grund\n",
    "\n",
    "Wir unterscheiden jetzt _Stemmer_, Programme, die den Stamm eines Wortes suchen, und _Lemmatisierer_, die das Lemma für jedes Wort suchen. Manche Stemmer trennen auch produktive Derivationssuffixe ab, und geben in vielen Fällen nicht den Stamm, sondern den Wurzel eines Wortes. Es wird oft davon ausgegangen, dass dies für Information Retrieval von Vorteil ist. Wenn man beispielsweise nach _analysieren_ sucht, möchte man wahrscheinlich nicht nur Ergebnisse mit _analysiere_, _analysiert_ , _analysierende_, usw. haben, sondern vermutlich auch welche, in denen nur das Wort _Analyse_ vorkommt. Man kann aber genau so Negativbeispiele finden. Ob diese Art von Stemming wirklich nützlich ist für Information Retrieval, ist nicht eindeutig gekärt (vgl. z.B.:  _BRANTS, Thorsten. Natural Language Processing in Information Retrieval. In: CLIN. 2003._).\n",
    "\n",
    "Ein guter Lemmatizer, der im NLTK enthalten ist, ist der WordNet-Stemmer, der die Vollformen einfach im online-Wörterbuch WordNet nachschlägt. Da ein Wort im Englischen oft zu mehreren Klassen gehören kann, braucht der Wordnet-Lemmatizer auch die Wortklasse. Wir brauchen jetzt ein paar Zeilen Code, um die Penn Treebank Tags in Wordnet-Klassen zu übersetzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T00:07:05.449200Z",
     "start_time": "2017-10-12T00:07:05.422200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hanover', 'or', 'Hannover', '(', '/ˈhænoʊvər/', ';', 'German', ':', 'Hannover', ',', 'pronounce', '[', 'haˈnoːfɐ', ']', '(', 'About', 'this', 'sound', 'listen', ')', ')', ',', 'on', 'the', 'River', 'Leine', ',', 'be', 'the', 'capital', 'and', 'large', 'city', 'of', 'the', 'German', 'state', 'of', 'Lower', 'Saxony', '(', 'Niedersachsen', ')', ',', 'and', 'be', 'once', 'by', 'personal', 'union', 'the', 'family', 'seat', 'of', 'the', 'Hanoverian', 'Kings', 'of', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Ireland', ',', 'under', 'their', 'title', 'as', 'the', 'duke', 'of', 'Brunswick-Lüneburg', '(', 'later', 'describe', 'as', 'the', 'Elector', 'of', 'Hanover', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "#treetagger\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "\n",
    "lemmata = [lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in tags]\n",
    "print(lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas einfacher ist der sogenannt _Porter Stemmer_. Der Porterstemmer benutzt kein Wörterbuch sondern hat nur eine Liste von Suffixen, die abgetrennt oder ersetzt werden. Dies führt in vielen Fällen zu unsinnigen Ergebnisse. Oft ist das aber unproblematisch, so lange verschiedene Formen eines Wortes auf dem gleichen eindeutigen Stamm zurückgeführt werden. Neben dem Porter Stemmer enthält NLTK den Lancaster Stemmer, der nach dem gleichen Prinzip arbeitet. Schauen Sie sich die Ergebnisse genau an und vergleichen die Sie die STämme mit den Lemmata des Wordnet-Stemmers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T00:14:12.233200Z",
     "start_time": "2017-10-12T00:14:12.221200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "['hanov', 'or', 'hannov', '(', '/ˈhænoʊvər/', ';', 'german', ':', 'hannov', ',', 'pronounc', '[', 'haˈnoːfɐ', ']', '(', 'about', 'thi', 'sound', 'listen', ')', ')', ',', 'on', 'the', 'river', 'lein', ',', 'is', 'the', 'capit', 'and', 'largest', 'citi', 'of', 'the', 'german', 'state', 'of', 'lower', 'saxoni', '(', 'niedersachsen', ')', ',', 'and', 'wa', 'onc', 'by', 'person', 'union', 'the', 'famili', 'seat', 'of', 'the', 'hanoverian', 'king', 'of', 'the', 'unit', 'kingdom', 'of', 'great', 'britain', 'and', 'ireland', ',', 'under', 'their', 'titl', 'as', 'the', 'duke', 'of', 'brunswick-lüneburg', '(', 'later', 'describ', 'as', 'the', 'elector', 'of', 'hanov', ')', '.']\n",
      "\n",
      "Lancaster Stemmer:\n",
      "['hanov', 'or', 'hannov', '(', '/ˈhænoʊvər/', ';', 'germ', ':', 'hannov', ',', 'pronount', '[', 'haˈnoːfɐ', ']', '(', 'about', 'thi', 'sound', 'list', ')', ')', ',', 'on', 'the', 'riv', 'lein', ',', 'is', 'the', 'capit', 'and', 'largest', 'city', 'of', 'the', 'germ', 'stat', 'of', 'low', 'saxony', '(', 'niedersachs', ')', ',', 'and', 'was', 'ont', 'by', 'person', 'un', 'the', 'famy', 'seat', 'of', 'the', 'hanov', 'king', 'of', 'the', 'unit', 'kingdom', 'of', 'gre', 'britain', 'and', 'ireland', ',', 'und', 'their', 'titl', 'as', 'the', 'duk', 'of', 'brunswick-lüneburg', '(', 'lat', 'describ', 'as', 'the', 'elect', 'of', 'hanov', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "stems = [porter.stem(t) for t in tokenized_text[0]]\n",
    "print(stems)\n",
    "\n",
    "print(\"\\nLancaster Stemmer:\")\n",
    "stems = [lancaster.stem(t) for t in tokenized_text[0]]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1f718d0b-7381-44ef-89e1-3f301ac85d38"
    }
   },
   "source": [
    "## Deutsch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2679e2d6-048a-408d-89c3-b20f85274a9b"
    }
   },
   "source": [
    "Deutsch und Englsich sind Sprachen, die sich in vielen Hinsichten zielich ähnlich sind. Die Verarbeitungsschritte für einen Deutschen Text unerscheiden sich daher nicht wesentlich von denen für englische Texte. Bei der Tokenisierung und Satzerkennung müssen wir lediglich Deutsch als Parameter angeben, damit besonderheiten des Deutschen besser berücksichtigt werden.\n",
    "\n",
    "Zum Ausprobieren, lesen wir wieder  einen kurzen Text ein. Den hier genutzten Beispieltext finden Sie hier: http://textmining.wp.hs-hannover.de/texte/syrien.txt . Der Text ist der Wikipediaseite http://de.wikipedia.org/wiki/Syrien.html entnommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T15:09:11.664000Z",
     "start_time": "2017-10-13T15:09:11.601000Z"
    },
    "nbpresent": {
     "id": "0055c2a0-0037-4240-89bf-34c724d5d182"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "['April', 'bis', 'Oktober', ')', 'Kfz-Kennzeichen', 'SYR', 'ISO', '3166', 'SYR', 'Internet-TLD', '.sy', 'Telefonvorwahl', '+963', 'Alle', 'Angaben', 'schließen', 'die', 'von', 'Israel', 'besetzten', 'Teile', 'der', 'Golanhöhen', 'mit', 'ein', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "textfile = codecs.open(\"texte/syrien.txt\", \"r\", \"utf-8\")\n",
    "text = textfile.read()\n",
    "textfile.close()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text,language='german')\n",
    "#print(sentences[20:23])\n",
    "print(len(sentences))\n",
    "tokenized_sent = nltk.tokenize.word_tokenize(sentences[4],language='german')\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aufgabe 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re \n",
    "import glob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wortartcountnouns(file):\n",
    "    textfile = codecs.open(file, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    wortlist = []\n",
    "    #print(text)\n",
    "    sentences = nltk.sent_tokenize(text,language='german')\n",
    "    #print(sentences[20:23])\n",
    "    #print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        tokenized_sent = nltk.tokenize.word_tokenize(sent,language='german')\n",
    "        #print(tokenized_sent)\n",
    "        wortartlist = tagger.tag_sent(tokenized_sent)\n",
    "        #print(wortartlist)\n",
    "        for (wort, lemma, pos) in wortartlist: \n",
    "            if 'NN'== pos or 'NE' ==pos:\n",
    "                #element = (wort, lemma, pos)\n",
    "                wortlist.append(wort)\n",
    "    #print(wortlist)\n",
    "            #tags = tagger.tag_sent(sent[index])\n",
    "            #print(tags)\n",
    "    count_nouns=Counter(wortlist)    \n",
    "    return count_nouns.most_common(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n",
      "[('A123Systems', 6), ('Millionen', 4), ('US-Dollar', 4), ('Massachusetts', 3), ('Unternehmen', 3), ('Dezember', 3), ('USABC', 3), ('Hersteller', 2), ('Insolvenz', 2), ('Wanxiang', 2)]\n",
      "[('Unternehmen', 5), ('A4Tech', 2), ('Deutschland', 2), ('Technology', 2), ('Corporation', 1), ('Peripheriegeräte', 1), ('Computer-Bereich', 1), ('Computermäuse', 1), ('Tastaturen', 1), ('Hauptsitz', 1)]\n",
      "[('Bank', 61), ('ABN', 57), ('AMRO', 53), ('Fortis', 19), ('Milliarden', 14), ('Euro', 14), ('LaSalle', 14), ('Oktober', 10), ('Fusion', 9), ('Niederlande', 8)]\n",
      "[('Abu', 11), ('Dhabi', 11), ('Al', 8), ('Mitglied', 7), ('ADNOC', 6), ('Unternehmen', 6), ('Company', 5), ('Sheikh', 5), ('Zayed', 5), ('Nahyan', 5)]\n",
      "[('Air', 7), ('DHL', 6), ('Airborne', 4), ('Boeing', 4), ('ABX', 3), ('Express', 3), ('Cargo', 3), ('Flotte', 3), ('Aviation', 3), ('Unternehmen', 2)]\n",
      "[('Acciona', 23), ('Unternehmen', 10), ('%', 10), ('Übernahme', 5), ('Endesa', 5), ('Aktivitäten', 5), ('Geschäftsbereich', 5), ('Hotel', 5), ('Gruppe', 4), ('y', 4)]\n",
      "[('Actelion', 8), ('Unternehmen', 4), ('Ltd', 3), ('Swiss', 3), ('Behandlung', 3), ('Hypertonie', 3), ('Entwicklung', 2), ('Vertrieb', 2), ('Index', 2), ('USA', 2)]\n",
      "[('Lighting', 7), ('Brands', 4), ('Acuity', 3), ('Unternehmen', 3), ('Schutzmarken', 2), ('Controls', 2), ('Hersteller', 1), ('Beleuchtungssystemen', 1), ('Hauptsitz', 1), ('Atlanta', 1)]\n",
      "[('Acxiom', 10), ('Deutschland', 6), ('Informationen', 5), ('Corporation', 3), ('USA', 3), ('Unternehmen', 3), ('GmbH', 3), ('Verfahren', 3), ('Adressen', 3), ('CRM', 3)]\n",
      "[('Kraftwerk', 4), ('Adani', 2), ('Power', 1), ('Stromversorger', 1), ('Group', 1), ('Unternehmen', 1), ('Kohlekraftwerke', 1), ('Mundra', 1), ('Leistung', 1), ('Tiroda', 1)]\n",
      "[('Firma', 7), ('Adaptec', 6), ('Milpitas', 2), ('Kalifornien', 2), ('SAN', 2), ('Software', 2), ('Shugart', 2), ('USB', 2), ('Elipsan', 2), ('Hardwarehersteller', 1)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "filelist = glob.glob('Firmen\\Firmen\\*.txt')\n",
    "print(len(filelist))\n",
    "#print(wortartcountnouns(filelist[10]))\n",
    "for file in filelist[:11]:\n",
    "    print(wortartcountnouns(file))\n",
    "    #datei = codecs.open(file,'r','utf8')\n",
    "    #print(file)\n",
    "    #for virus in datei:\n",
    "        #fund = re.search(r'\\b\\S*[Vv]ir(u|e)[sn]\\b', virus)\n",
    "        #if  fund:\n",
    "            #vlist.append(fund.group(0))\n",
    "            #i += 1\n",
    "            #print(vlist, i, '. satir ' )\n",
    "        #datei.close()\n",
    "#virusstatistik = Counter(vlist)\n",
    "#virusstatistik.most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    tokenized_sent = nltk.tokenize.word_tokenize(sent,language='german')\n",
    "    #print(tokenized_sent)\n",
    "    wortartlist = tagger.tag_sent(tokenized_sent)\n",
    "    #print(wortartlist)\n",
    "    for (wort, lemma, pos) in wortartlist: \n",
    "        if 'NN'== pos or 'NE' ==pos:\n",
    "            #element = (wort, lemma, pos)\n",
    "            wortlist.append(wort)\n",
    "#print(wortlist)\n",
    "        #tags = tagger.tag_sent(sent[index])\n",
    "        #print(tags)\n",
    "        \n",
    "        \n",
    "count_nouns=Counter(wortlist)    \n",
    "print(count_nouns.most_common())                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "18585409-71cf-4452-ad94-a12d187a9221"
    }
   },
   "source": [
    "# Lemmatisierung und Wortarterkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider enthält das NLTK Paket keine Lemmatisierer und Wortarterkenner (POS Tagger) für das Deutsche. \n",
    "\n",
    "Wir nutzen hier für beide Funtionen den Hanover Tagger (Siehe: [Christian Wartena (2019). A Probabilistic Morphology Model for German Lemmatization. In: Proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019): Long Papers. Pp. 40-49, Erlangen.](https://doi.org/10.25968/opus-1527) )\n",
    "\n",
    "Wir müssen den Hanover Tagger zunächst (einmalig) herunterladen und installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting HanTa\n",
      "  Using cached HanTa-0.2.1-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: HanTa\n",
      "Successfully installed HanTa-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install HanTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir binden das Modul ein und laden ein vortrainiertes Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können jetzt Wörter oder Sätze analysieren. Die Funktion _analyze()_ gibt ein Lemma und Wortart für eine Wortform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fachmarkt', 'NN')\n"
     ]
    }
   ],
   "source": [
    "print(tagger.analyze('Fachmärkte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion _tag sent()_ werden alle Wörter in einem Satz lemmatisiert und und getagt. Bei Mehrdeutigkeiten, wir die Wortart gewählt, die im Kontext am wahrscheinlichsten ist. Wir versuchen das mal mit unserem Beispielsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embassy', 'of', 'the', 'United', 'States', ',', 'Damascus']\n",
      "[('Embassy', 'embassy', 'FM'), ('of', 'of', 'FM'), ('the', 'the', 'FM'), ('United', 'United', 'NE'), ('States', 'State', 'NE'), (',', '--', '$,'), ('Damascus', 'Damascu', 'NE')]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sent)\n",
    "tags = tagger.tag_sent(tokenized_sent)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'virusstatistik' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3096/1830046641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#datei.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mvirusstatistik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mvirusstatistik\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'virusstatistik' is not defined"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import re \n",
    "import glob\n",
    "from collections import Counter\n",
    "i = 0\n",
    "vlist = [] #Liste mit alle gefundene Virüs-Namen\n",
    "vcount = Counter()  # Wir zählen später die Häufigkeit jedes Virüs-Namen\n",
    "\n",
    "filelist = glob.glob('2. Reguläre Ausdrücke-20220906\\Infekte\\*.txt')\n",
    "#print(filelist)\n",
    "for file in filelist:\n",
    "    datei = codecs.open(file,'r','utf8')\n",
    "    #print(file)\n",
    "    for virus in datei:\n",
    "        fund = re.search(r'\\b\\S*[Vv]ir(u|e)[sn]\\b', virus)\n",
    "        if  fund:\n",
    "            vlist.append(fund.group(0))\n",
    "            i += 1\n",
    "            #print(vlist, i, '. satir ' )\n",
    "        #datei.close()\n",
    "    virusstatistik = Counter(vlist)\n",
    "virusstatistik.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere Möglichkeiten des Hanover Taggers werden [hier](https://github.com/wartaal/HanTa/blob/master/Demo.ipynb) beschrieben."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "159px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
