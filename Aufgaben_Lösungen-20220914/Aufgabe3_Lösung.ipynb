{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob # Liste von Datei \n",
    "import nltk # für Tokenization\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from HanTa import HanoverTagger as ht\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz') # für POS-Tagging. \n",
    "\n",
    "dateilist=glob.glob('Firmen/*.txt') # List mit Daten in Firmen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Firmen\\\\A123_Systems.txt',\n",
       " 'Firmen\\\\A4Tech.txt',\n",
       " 'Firmen\\\\ABN_AMRO.txt',\n",
       " 'Firmen\\\\Abu_Dhabi_National_Oil_Company.txt',\n",
       " 'Firmen\\\\ABX_Air.txt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateilist[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung ohne Funktion : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen', 5), ('A4Tech', 2), ('Deutschland', 2), ('Technology', 2), ('Corporation', 1), ('Peripheriegeräte', 1), ('Computer-Bereich', 1), ('Computermäuse', 1), ('Tastaturen', 1), ('Hauptsitz', 1)]\n"
     ]
    }
   ],
   "source": [
    "# für den Text: 'Firmen\\\\A4Tech.txt'\n",
    "datei=codecs.open('Firmen\\\\A4Tech.txt','r','utf8') #datei aufmachen\n",
    "text=datei.read()  # datei lesen\n",
    "datei.close() #datei zumachen\n",
    "\n",
    "nouns=Counter() # Leerer Counter\n",
    "\n",
    "sentences=nltk.sent_tokenize(text,language='german') # List mit die Sätzen des Textes  \n",
    "\n",
    "for sent in sentences:\n",
    "    worter=nltk.word_tokenize(sent,language='german') # Durch Tokenization erkenne wir die Wörter des Satzes. \n",
    "    tags=tagger.tag_sent(worter)  # Jetzt nehmen wir die Wörter und erfahren wir die TAGS. Ein TAG ist (wort,lemma, pos). pos=part of speech\n",
    "    nouns_sent=[wort for (wort,lemma,pos) in tags if pos in ['NN','NE']] # wenn das Wort ein Nomen ist, zahlen wir das Wort mit dem Counter unten. \n",
    "    nouns.update(nouns_sent) #Ja, hier zahlen wir.\n",
    "print(nouns.most_common(10)) #most_common zeigt nur die Häufgsten Nomen. In diesem Fall die 10 häufgsten Nomen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lösung mit  Funktion definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsere Funktion heißt noun_text(). Das INput ist datei.\n",
    "def noun_text(datei):\n",
    "    datei=codecs.open(datei,'r','utf8') #datei aufmachen\n",
    "    text=datei.read()  # datei lesen\n",
    "    datei.close() #datei zumachen\n",
    "\n",
    "    nouns=Counter() # Leerer Counter\n",
    "\n",
    "    sentences=nltk.sent_tokenize(text,language='german') # List mit die Sätzen des Textes  \n",
    "\n",
    "    for sent in sentences:\n",
    "        worter=nltk.word_tokenize(sent,language='german') # Durch Tokenization erkenne wir die Wörter des Satzes. \n",
    "        tags=tagger.tag_sent(worter)  # Jetzt nehmen wir die Wörter und erfahren wir die TAGS. Ein TAG ist (wort,lemma, pos). pos=part of speech\n",
    "        nouns_sent=[wort for (wort,lemma,pos) in tags if pos in ['NN','NE']] # wenn das Wort ein Nomen ist, zahlen wir das Wort mit dem Counter unten. \n",
    "        nouns.update(nouns_sent) #Ja, hier zahlen wir.\n",
    "    #anstatt print, bei Funktion zeigen wir das Ergebnis mit 'return'\n",
    "    return(nouns.most_common(10)) #most_common zeigt nur die Häufgsten Nomen. In diesem Fall die 10 häufgsten Nomen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Unternehmen', 5),\n",
       " ('A4Tech', 2),\n",
       " ('Deutschland', 2),\n",
       " ('Technology', 2),\n",
       " ('Corporation', 1),\n",
       " ('Peripheriegeräte', 1),\n",
       " ('Computer-Bereich', 1),\n",
       " ('Computermäuse', 1),\n",
       " ('Tastaturen', 1),\n",
       " ('Hauptsitz', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jetzt laufen wir die Funktion für beliebete Datei. Aber hier wieder 'Firmen\\\\A4Tech.txt'\n",
    "noun_text('Firmen\\\\A4Tech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
